-
  layout: lecture
  selected: y
  date: 2025-10-28
  img: introduction-icon_1-267x300
  uid: intro
  title: "Introduction"
  instructor: "Ekaterina Shutova and Wilker Aziz"
  note: 
  abstract: >
    "Introduction to the course"
  background: 
  discussion:
  slides: resources/slides/NLP1-2024-introduction.pdf
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=94ab0f83-8a91-469a-b503-b21300905a5b
  further: 
    - "Chapter B: [Naive Bayes classification and sentiment](https://web.stanford.edu/~jurafsky/slp3/B.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2025-10-30
  img: NGramLM-bayesnet
  uid: lec2
  title: "Language modelling"
  instructor: "Wilker Aziz"
  abstract: >
    "In this lecture, we will discuss language models, i.e. modelling word sequences, using statistical techniques."
  note: Do check the _background_ and _further reading_ for required and suggested self-study material.
  background: 
    - "Required self-study: [representing and estimating tabular Categorical distributions](https://youtu.be/1vE8zKj0-GI)"
    - "Optional, but highly recommended: it can be useful to learn a little bit about directed graphical models and tabular representation of conditional probability distributions. Check this [video](https://www.youtube.com/watch?v=9lmFfhzpWag) and [slides](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa29uMS0tU2xqWk04NzEwVzJ2Ukc0Wm1CRTBSQXxBQ3Jtc0tuSkZxcmdGQmcwenpMeU5lZVVUbXJKVXZtUEVORmw1R0FEV1dIZ09la2Q5bWRhUllsM25KeHJwSkJrNnhncWIzcnZ1ci1qMWo3UUVTOXdCeWVISFk5Y01TaER0RHdIWHF5Z0hiZzRMcDZ5Ml8zRTJkMA&q=https%3A%2F%2Fwilkeraziz.github.io%2Fassets%2Fpdfs%2Fslides-appendix-PGM.pdf&v=9lmFfhzpWag)"
  discussion:
  mentimeter: https://www.mentimeter.com/app/presentation/n/al5s98g69c7vxxyhmq56636fgxwmvr8o/edit?source=share-modal 
  slides: resources/slides/NLP1-language-modelling.pdf
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=7af18a66-d2ec-4ade-b15f-b213009060ce 
  further: 
    - "Required - Section 3.2 (evaluation LMs) from [Language modelling with n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf) in Jurafsky and Martin (3rd edition)."
    - "Optional - Chapter 3: [Language modelling with n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf) in Jurafsky and Martin (3rd edition)."    
    - "Necessary for PnP1 - How to sample from a Categorical distribution: [video](https://youtu.be/49a378GuMXM?si=exGGZnbiNpyrby3W) or [section 3.4 of textbook](https://web.stanford.edu/~jurafsky/slp3/3.pdf) "
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-11-04
  img: PoS
  uid: lec3
  title: "Sequence labelling"
  instructor: "Wilker Aziz"
  note: Do check the _background_ and _further reading_ for required and suggested self-study material.
  abstract: >
    "In this lecture, we will discuss labelling tasks such as part-of-speech tagging, named-entity recognition and semantic role labelling."
  background:
    - "Required self-study: representing and estimating logistic Categorical distributions ([part 1/2 theory](https://youtu.be/5oz2vAzAs8k), [part 2/2 example](https://youtu.be/DsflDpm5h-c))"
  discussion:
  mentimeter: 
  slides: resources/slides/NLP1-sequence-labelling.pdf
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1931cb98-dfff-4ae3-aa11-b21300906689 
  further:     
    - "Required self-study: [value recursion for HMMs](https://youtu.be/rVCd7NrGcSI)"
    - "Play with the [Colab example](https://colab.research.google.com/drive/19LiLszW61-XCvMMdyrQKq8x_hVqEpM_P?usp=sharing) if you like"
    - "Chapter 17.2: [Part-of-speech tagging](https://web.stanford.edu/~jurafsky/slp3/17.pdf) in Jurafsky and Martin (3rd edition)."
  code: https://colab.research.google.com/drive/19LiLszW61-XCvMMdyrQKq8x_hVqEpM_P?usp=sharing
  data: 
-
  layout: lecture
  selected: y
  date: 2025-11-06
  img: Parsing
  uid: lec4
  title: "Modelling syntactic structure"
  instructor: "Wilker Aziz"
  note: Do check _further reading_ for required and suggested self-study material.
  abstract: >
    "In this lecture, we will introduce syntax and ideas to model it."
  background:
  discussion:
  mentimeter: https://www.mentimeter.com/app/presentation/n/ali5ndcorxoib77eij5r3t4n637ow6y9/edit?source=share-modal 
  slides: resources/slides/NLP1-modelling-syntactic-structure.pdf 
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1388f80a-a124-4af9-ad68-b21300906b9d
  code: https://colab.research.google.com/drive/11UGhml-TKGh10eKDF5y5-ZSHzwSNvvfF?usp=sharing
  further: 
    - "Required: [Katia's class on morphology (first 43 minutes of the video)](https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=ddcd1207-fdfd-4af1-8f09-b153005e380a&start=172.733)"
    - "Required: check this video on [dynamic programming algorithms for PCFGs](https://youtu.be/jM0x_YMmEMM)."
    - "Recommended [Section 18.6](https://web.stanford.edu/~jurafsky/slp3/18.pdf) of textbook."
    - "Recommended [Chapter E - statistical constituency parsing](https://web.stanford.edu/~jurafsky/slp3/E.pdf)"
  data:  
-
  layout: lecture
  selected: y
  date: 2025-11-11
  img: skip-gram
  uid: lec5
  title: "Lexical semantics and word embeddings"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this lecture, we will introduce statistical models of word meaning, discuss generalisation from words to semantic classes and learning dense vector representations - word embeddings."
  background:
  discussion:
  slides: resources/slides/NLP1-lexical-semantics.pdf
  further: 
    - "Chapter I: [Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/I.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 5: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/5.pdf) in Jurafsky and Martin (3rd edition)."
    - "A gentle introduction to neural networks can be found [here](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
    - "The following paper provides a nice explanation of skip-gram with negative sampling: Yoav Goldberg and Omer Levy. [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)"
    - "[This paper](https://aclanthology.org/P14-2050.pdf) experiments with using syntactic dependency-based contexts to train the skip-gram model and provides some analysis on what kind of information the context vectors encode."
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=050e1289-a29f-4b7a-a1dd-b2130090714f&start=432.230037
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2025-11-13
  img: srn
  uid: lec6
  title: "Compositional semantics and sentence representations"
  instructor: "Vera Neplenbroek"
  note: 
  abstract: >
    "In this lecture, we will discuss compositional semantics, i.e. modelling the meaning of phrases and sentences, and learning neural representations of sentences."
  background:
  discussion:
  slides: resources/slides/NLP1-2024-Lecture-6_Compositional-semantics-and-sentence-representations.pdf
  further: 
    - "Chapter 6: [Neural networks](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 13: [RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/13.pdf) in Jurafsky and Martin (3rd edition)."
    - "A good and general reference for Neural Networks in NLP: Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726)"
    - "A gentle introduction to LSTMs is available [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
    - "This is one of the papers that have introduced tree LSTM models: Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf)" 
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=82bca1cf-4dcf-48a8-8a5d-b228010220b9
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2025-11-18
  img: Discourse
  uid: lec7
  title: "Discourse processing"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this lecture, we will discuss discourse processing, i.e. modelling larger text fragments."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture7.pdf
  further: 
    - "Chapter 23: [Coreference Resolution and Entity Linking](https://web.stanford.edu/~jurafsky/slp3/23.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 24: [Discourse coherence](https://web.stanford.edu/~jurafsky/slp3/24.pdf) in Jurafsky and Martin (3rd edition)." 
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=04002635-f37f-4bab-ae49-b21300907adf&start=0.894223
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-11-25
  img: BERT
  uid: lec8
  title: "Large language models"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this session, we will discuss large language models, their pretraining strategies, their architectures and their applications."
  background:
  discussion:
  slides: resources/slides/NLP1-lecture8-LLMs.pdf
  further:
    - "The slides list a number of papers that constitute the further reading material for this lecture."
    - "The [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) blog provides a very nice introduction to the Transformer architecture."
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=ae0cccb9-7738-4388-85d0-b21300908307&start=246.417569
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-11-27
  img: Twitter
  uid: lec9
  title: "Interpretability of NLP models"
  instructor: "Michael Hanna"
  note: 
  abstract: >
    "We will discuss techniques that allow us to better understand what kind of information neural language models encode and how they do it."
  background:
  discussion:
  slides: resources/slides/NLP1_Intro_to_Interpretability.pdf
  further: 
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=06bbec2e-4070-4bf3-bcbb-b21300908779
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-12-02
  img: MT
  uid: lec10
  title: "LLM safety and alignment"
  instructor: "Guest lecture by Pushkar Mishra (DeepMind)"
  note: 
  abstract: >
    "We will discuss techniques for making LLM outputs safe, including safety fine-tuning and alignment to human preferences."
  background:
  discussion:
  slides: resources/slides/LLM-safety.pdf
  further: 
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=c45ae980-5185-4cd0-98f7-b21300908d30&start=1435.83981
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-12-04
  img: dialogue
  uid: lec11
  title: "Dialogue modelling"
  instructor: "Raquel Fernandez"
  note: 
  abstract: >
    "In this lecture, we will introduce an important NLP application -- dialogue modelling."
  background:
  discussion:
  slides: resources/slides/NLP1-DialogueModelling-2024.pdf
  further:
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=7c1632bb-4e5a-4013-a04e-b213009091d0
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2025-12-9
  img: Summarization
  uid: lec12
  title: "Neural sequence modelling"
  instructor: "Wilker Aziz"
  note: 
  abstract: >
    "In this lecture, we will talk about neural models of sequential data. Examples will include sequence labelling, machine translation and text summarisation."
  background:
  discussion:
  slides: resources/slides/NLP1-neural-sequence-modelling.pdf
  further: 
    - "An introduction to sequence-to-sequence models: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a138b283-fbe5-4e40-aa59-b2130090969c 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2025-12-11
  img: Live-1
  uid: lec-final
  title: "Summary of the course and Q&A"
  instructor: "Ekaterina Shutova and Wilker Aziz"
  note: 
  abstract: >
    "In this lecture, we will give you a brief summary of the course and how different areas we have studied interface. We will also discuss what to expect at the exam."
  background:
  discussion:
  slides: resources/slides/NLP1_2024_Summary_lecture.pdf
  video: https://hva-uva.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=af61cbbb-9f07-4de6-8976-b19b00ffcb3f
  further:
  code: 
  data:   

